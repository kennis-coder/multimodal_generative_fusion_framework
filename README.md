# A Deep Multimodal Generative  and Fusion Framework for Class-imbalanced Multimodal Data
The purpose of multimodal classification is to integrate different feature sources to make a decision. Interactions between different modalities are crucial to such a task. However, common strategies in previous studies have been either to concatenate features from various information sources into a compound vector or to input them separately into several different classifiers that are then assembled into a strong classifier for the final prediction. Both approaches weakened even ignored the interactions among different feature modalities. Besides, if confronted with class-imbalanced data, multimodal classification becomes troublesome. In this study, we proposed a deep multimodal generative and fusion framework for multimodal classification with class-imbalanced data. Notably, a deep multimodal generative adversarial network (DMGAN) handles this class imbalance problem. A deep multimodal hybrid fusion network (DMHFN) finds fine-grained interactions and integrates different information sources for multimodal classification. Compared to several start-of-the-art methods, experiments on a faculty homepage dataset we collected show the superiority of our framework.